{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data :  (647054, 7)\n",
      "Test  data :  (653646, 6)\n",
      "submission  data :  (95674, 39)\n"
     ]
    }
   ],
   "source": [
    "# Import the functions used in this project\n",
    "# from private_pkg.functions import *\n",
    "# import featureMatrixFunction as fm\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "train = pd.read_csv(\"../asset/train.csv\")\n",
    "test = pd.read_csv(\"../asset/test.csv\")\n",
    "submission = pd.read_csv(\"../asset/sample_submission.csv\")\n",
    "\n",
    "# Success - Display the first record\n",
    "print(\"Train data : \", train.shape)\n",
    "print(\"Test  data : \", test.shape)\n",
    "print(\"submission  data : \", submission.shape)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle as pkl\n",
    "slack_url = pkl.load(open(\"Slack_url/send_url.pickle\", \"rb\"))\n",
    "\n",
    "# 원본을 유지하기 위해서 카피\n",
    "df_train = train.copy()\n",
    "df_test = test.copy()\n",
    "df_submission = submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting awesome_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile awesome_functions.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests # to send slack msg\n",
    "import json # to send slack msg\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\"\"\"\n",
    "    make_submission_df : Walmart에서 제공한 Submission df 원본과 predict_proba 리스트를 넣어주면 Submission할 수 있는 형태의 Df를 반환한다.\n",
    "    get_df_to_fit : make_df_we_wanted로 만든 매트릭스를 바로 머신러닝 모델에 Fit할 수 있는 상태로 만들어 반환한다.\n",
    "    make_df_we_wanted : 우리가 원하는 모형의 Df를 반환한다. (DD, FL)두가지 컬럼만 지원한다.\n",
    "    compareClassificationReport : 독립된 두개의 Classification_report를 패러미터로 넣어주면 각 TripType별로 확인할 수 있는 Df를 반환한다.\n",
    "    getAccuracy : y_true, y_pred, length of data를 넣어주면 Accuracy를 반환한다.\n",
    "    saveDataFrameToCsv : 데이터 프레임을 저장해준다. \n",
    "    sendSlackDm : 슬랙 url을 이용해서 메시지를 발송한다.\n",
    "    saveModelObjectAsPickle : 모델 객체를 저장하는 함수.\n",
    "\"\"\"\n",
    "\n",
    "def make_submission_df(df, y_pred):\n",
    "    \"\"\"\n",
    "        Submission용 df를 만드는 함수\n",
    "        df : walmart에서 제공한 submission df를 넣어준다.\n",
    "        y_pred : df의 visit_number와 같은 순서로 나열된 predict한 y값으로 이루어진 List를 넣어준다.\n",
    "        간단하게 말하자면 그냥 model.predict(x)해서 나온 y 리스트 넣어주면 된다.\n",
    "    \"\"\"\n",
    "    y_pred_proba_xgb_df = pd.DataFrame(y_pred, columns = df.columns[1:])\n",
    "    result_df = pd.concat([df[\"VisitNumber\"], y_pred_proba_xgb_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def get_df_to_fit(df, is_test_df = False):\n",
    "    \n",
    "    # test 모델은 TripType이 없으므로, X만 반환한다.\n",
    "    if is_test_df:\n",
    "        return df.drop([\"Return\", \"VisitNumber\"], axis = 1)\n",
    "    \n",
    "    # model에 fit할 때 사용할 X, y를 반환한다. (X, y로 받아줘야 한다.)\n",
    "    return df.drop([\"Return\", \"TripType\", \"VisitNumber\"], axis = 1), df[\"TripType\"]\n",
    "\n",
    "def make_df_we_wanted(df, df_train, df_test, dummie_col = \"DepartmentDescription\", \n",
    "                      is_use_positive_scancount_only = True, is_test_df = False, is_need_null_column = False):\n",
    "    \"\"\"\n",
    "        df : 전처리 목표 dataframe\n",
    "        df_train : walmart에서 제공한 train dataframe을 넣어준다. (FinelineNumber를 이용해서 만들 때 필요하다.)\n",
    "        df_test : walmart에서 제공한 test dataframe을 넣어준다. (FinelineNumber를 이용해서 만들 때 필요하다.)\n",
    "        dummie_col : default는 DepartmentDescription이지만, FinelineNumber가 필요할 때는 문자열로 넣어주면된다.\n",
    "        is_use_positive_scancount_only : False를 할 경우엔 음수도 같이 누적된다.\n",
    "        is_test_df : default는 False. model.predict에 넣을 test df를 만들 때 True로 넣어준다.\n",
    "        is_need_null_column : default는 False. True를 넣어주면 NaN인 아이템을 산 데이터 정보는 Null이라는 컬럼을 만들어 넣어준다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 넣어준 패러미터들 정보에대해 Display한다.\n",
    "    __display_parameter_detail(dummie_col, is_use_positive_scancount_only, is_test_df, is_need_null_column)\n",
    "    \n",
    "    result = df.copy()\n",
    "    \n",
    "    # Null데이터 개수도 필요한 경우에 사용하면 컬럼으로 dd인경우 Null이 추가되고 fl인 경우엔 -1컬럼이 추가된다.\n",
    "    if is_need_null_column: \n",
    "        if dummie_col == \"DepartmentDescription\":\n",
    "            result[dummie_col] = result[dummie_col].apply(lambda a : \"Null\" if type(a) is float else a)\n",
    "        else:\n",
    "            result[dummie_col] = result[dummie_col].apply(lambda a : -1.0 if np.isnan(a) else a)\n",
    "    \n",
    "    # 요일을 숫자로 변경\n",
    "    result[\"Weekday\"] = __change_weekday_to_number(result)\n",
    "\n",
    "    # 반환여부를 1(반환한 경우), 0로 표현\n",
    "    result[\"Return\"] = result[\"ScanCount\"].apply(lambda a: 1 if a < 0 else 0)\n",
    "    \n",
    "    if dummie_col == \"DepartmentDescription\":\n",
    "        # 원하는 컬럼(dummie_col에 넣어준 컬럼명)을 Dummie로 변경\n",
    "        result = __make_dummy_columns(result, dummie_col)\n",
    "    else:\n",
    "        return __make_fl_df_using_for_sentence(result, df_train, df_test, dummie_col,\\\n",
    "                                               is_use_positive_scancount_only, is_test_df, is_need_null_column)\n",
    "    \n",
    "    # VisitNumber를 이용해서 groupby하여 Row수를 VisitNumber Unique한 숫자만큼 축소\n",
    "    result = __make_df_groupby_visit_number(result, is_test_df, is_use_positive_scancount_only)\n",
    "    \n",
    "    return __make_weekday_as_dummies(result)\n",
    "\n",
    "def __display_parameter_detail(dummie_col, is_use_positive_scancount_only, is_test_df, is_need_null_column):\n",
    "    display(Markdown(\"##### Dummy타입으로 만든 컬럼 명 : \" + dummie_col))\n",
    "    display(Markdown(\"##### ScanCount는 양수만 사용\")) if is_use_positive_scancount_only else display(Markdown(\"##### ScanCount는 음수만 사용\"))\n",
    "    display(Markdown(\"##### Test df 만드는 중\")) if is_test_df else display(Markdown(\"##### Train df 만드는 중\"))\n",
    "    display(Markdown(\"##### Null 컬럼을 만듬\")) if is_need_null_column else display(Markdown(\"##### Null 컬럼 없는 모델\"))\n",
    "    print()\n",
    "    if not is_test_df:\n",
    "        display(Markdown(\"> 위 정보들을 Display하는 이유는 이번 FeatureMatrix를 사용한 모델에 Fit할 Test 모델 만들 때 같은 전처리를 하기 위해서다.\"))\n",
    "\n",
    "def __change_weekday_to_number(df):\n",
    "    weekday_dict = {\n",
    "        \"Monday\" : 1,\n",
    "        \"Tuesday\" : 2,\n",
    "        \"Wednesday\" : 3,\n",
    "        \"Thursday\" : 4,\n",
    "        \"Friday\" : 5,\n",
    "        \"Saturday\" : 6,\n",
    "        \"Sunday\" : 7\n",
    "    }\n",
    "    return df[\"Weekday\"].map(weekday_dict)\n",
    "\n",
    "def __make_dummy_columns(df, dummie_col):\n",
    "    ItemNumber = df[\"ScanCount\"]\n",
    "    dummies_desc = pd.get_dummies(df[dummie_col])\n",
    "    desc_cols = dummies_desc.columns\n",
    "    df[desc_cols] = dummies_desc.apply(lambda x: x * ItemNumber)\n",
    "    should_remove_cols = [\"Upc\", \"DepartmentDescription\", \"FinelineNumber\", \"ScanCount\"]\n",
    "    new_cols = [col for col in df.columns if col not in should_remove_cols]\n",
    "    return df[new_cols]\n",
    "\n",
    "def __make_fineline_number_dummy_columns(df, df_train, df_test, dummie_col):\n",
    "    train_fl_li = [fl if not np.isnan(fl) else -1  for fl in df_train[dummie_col].unique()]\n",
    "    test_fl_li = [fl if not np.isnan(fl) else -1  for fl in df_test[dummie_col].unique()]\n",
    "    fl_cols = [li if not np.isnan(li) else -1.0 for li in list(set(list(test_fl_li) + list(train_fl_li)))]\n",
    "    df = __make_dummy_columns(df, dummie_col)\n",
    "    return df, fl_cols\n",
    "\n",
    "def __make_df_groupby_visit_number(df, is_test_df, is_use_positive_scancount_only):        \n",
    "    cols = [col for col in list(df.columns) if col != \"VisitNumber\"]\n",
    "    np_max_cols = cols[:2] if is_test_df else cols[:3]\n",
    "    values = [np.max if col in np_max_cols else np.sum for col in cols]\n",
    "    dict_ = dict(zip(cols, values))\n",
    "    result_df = df.groupby(by='VisitNumber').agg(dict_).reset_index()\n",
    "    \n",
    "    # is_use_positive_scancount_only가 True인 경우에는 0이하의 ScanCount는 모두 0으로 만들어준다.\n",
    "    if is_use_positive_scancount_only:\n",
    "        result_df = pd.DataFrame(np.where(result_df < 0, 0, result_df), columns=result_df.columns)\n",
    "    if is_test_df:\n",
    "        # test경우에는 없는 컬럼이므로 0으로 채워서 추가해준다. (Model을 만들 때 사용한 feature_matrix와 shape이 같아야하므로)\n",
    "        result_df[\"HEALTH AND BEAUTY AIDS\"] = np.zeros(len(result_df))\n",
    "        return result_df\n",
    "    return result_df\n",
    "\n",
    "def __make_weekday_as_dummies(df):\n",
    "    # finelinenumber를 column으로 만든경우에는 1,2,3,4,5,6,7 중에 몇개가 중복될 위험이 있어서 다시 명시적으로 바꿔준다.\n",
    "    weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    dummies_desc = pd.get_dummies(df[\"Weekday\"])\n",
    "    dummies_desc.columns = weekdays\n",
    "    desc_cols = dummies_desc.columns\n",
    "    df[desc_cols] = dummies_desc\n",
    "    return df.drop(\"Weekday\", axis = 1)\n",
    "\n",
    "def __make_fl_df_using_for_sentence(df, df_train, df_test, dummie_col, is_use_positive_scancount_only, is_test_df, is_need_null_column):\n",
    "    df[dummie_col] = df[dummie_col].apply(lambda a : -1.0 if np.isnan(a) else a)\n",
    "    train_fl_li = [fl if not np.isnan(fl) else -1  for fl in df_train[dummie_col].unique()]\n",
    "    test_fl_li = [fl if not np.isnan(fl) else -1  for fl in df_test[dummie_col].unique()]\n",
    "    dd_cols = [li if not np.isnan(li) else -1.0 for li in list(set(list(test_fl_li) + list(train_fl_li)))]\n",
    "    dd_cols.insert(0, \"Return\")\n",
    "    dd_cols.insert(0, \"VisitNumber\")\n",
    "    if not is_test_df:\n",
    "        dd_cols.insert(2, \"TripType\")\n",
    "        df = pd.DataFrame(df.groupby([\"VisitNumber\", \"TripType\", \"Weekday\", dummie_col]).sum()[\"ScanCount\"]).reset_index()\n",
    "    else:\n",
    "        df = pd.DataFrame(df.groupby([\"VisitNumber\", \"Weekday\", dummie_col]).sum()[\"ScanCount\"]).reset_index()\n",
    "    return __makeDf(df, dd_cols, is_test_df, dummie_col, is_use_positive_scancount_only)\n",
    "\n",
    "def __makeDf(df, dd_cols, is_test_df, dummie_col, is_use_positive_scancount_only):\n",
    "    display(Markdown(\"##### 이 작업은 데이터프레임을 반환하는 \\\n",
    "    함수이므로 변수로 받아주셔야합니다!! 오래 걸리는 작업이므로 지금 실수하셨다면 빨리 커널 종료하고 다시 시도해주세요.\"))\n",
    "    vn_uq_li = df[\"VisitNumber\"].unique()\n",
    "    \n",
    "    # 아래 for문에서는 FinelineNumber에 대해서만 일처리를 하기때문에 마지막에 Weekday부분은 Concat해야된다.\n",
    "    df = __make_weekday_as_dummies(df)\n",
    "    weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    values = [np.max for col in weekdays]\n",
    "    dict_ = dict(zip(weekdays, values))\n",
    "    df_to_concat_later = df.groupby(by=\"VisitNumber\").agg(dict_).reset_index().drop(\"VisitNumber\", axis = 1)\n",
    "    \n",
    "    li = []\n",
    "    for i, vn in enumerate(vn_uq_li):\n",
    "        tmp_df = df[df[\"VisitNumber\"] == vn]\n",
    "        space = np.zeros(len(dd_cols)).astype(int) if not is_test_df else np.zeros(len(dd_cols)).astype(int)\n",
    "        # 위에서 VisitNumber는 0으로 설정했으므로 0번 인덱스에 넣어준다.\n",
    "        space[0] = vn\n",
    "        # 위에서 Return 여부는 1으로 설정했으므로 1번 인덱스에 넣어준다. (Return했다면 1)\n",
    "        space[1] = 1 if True in (0 > tmp_df[\"ScanCount\"].unique()) else 0\n",
    "\n",
    "        # Test_df가 아닌 경우에는 TripType도 넣어줘야한다. Fit이 가능하도록.\n",
    "        if not is_test_df:\n",
    "            tripType = tmp_df[\"TripType\"].unique()[0]\n",
    "            # 위에서 TripType은 2번으로 설정했으므로 2번 인덱스에 넣어준다.\n",
    "            space[2] = tripType\n",
    "            \n",
    "        for row_nbr in tmp_df.index:\n",
    "            dd = tmp_df.loc[row_nbr][dummie_col]\n",
    "            scan_cnt = tmp_df.loc[row_nbr][\"ScanCount\"]\n",
    "            idx = dd_cols.index(dd)\n",
    "            if not is_use_positive_scancount_only:\n",
    "                space[idx] = scan_cnt\n",
    "            else:\n",
    "                if scan_cnt > 0:\n",
    "                    space[idx] = scan_cnt\n",
    "        li.append(space)\n",
    "        if (i % 5000) == 0:    \n",
    "            print(str(i) + \"명 진행됨. 아직 \" + str(len(vn_uq_li) - i) + \"명 데이터 남음.\")\n",
    "    \n",
    "    df_fl = pd.DataFrame(li, columns=dd_cols) \n",
    "    \n",
    "    return pd.concat([df_fl, df_to_concat_later], axis = 1)\n",
    "\n",
    "\n",
    "########################################################################################################################################\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost\n",
    "\n",
    "def compareClassificationReport(report1, report2):\n",
    "    \"\"\"\n",
    "        classification_report 두개를 비교 분석하기 용이하게 DF을 만들어서 반환한다.\n",
    "        report1 : classification_report\n",
    "        report2 : classification_report\n",
    "        set_trip_type_as_index : triptype을 인덱스로한 df를 원하는지 넣어준다. default는 True\n",
    "    \"\"\"\n",
    "    report1_df, cols = __preproccessToMakeDf(report1)\n",
    "    report2_df, cols = __preproccessToMakeDf(report2)\n",
    "    li = np.zeros(39 * 2 * 6).astype(str).reshape(39 * 2, 6)\n",
    "    cols.append(\"model\")\n",
    "    for idx in range(len(li)):\n",
    "        if idx % 2 == 0:\n",
    "            tmp = list(report_fl_np[idx//2])\n",
    "            tmp.append(\"fl\")\n",
    "            li[idx] = np.array(tmp)\n",
    "        else:\n",
    "            tmp = list(report_dd_np[idx//2])\n",
    "            tmp.append(\"dd\")\n",
    "            li[idx] = np.array(tmp)\n",
    "    df_report = pd.DataFrame(li, columns=cols)\n",
    "    for tt in df_report[\"TripType\"].unique():\n",
    "        display(df_report[df_report[\"TripType\"] == tt])\n",
    "    return df_report\n",
    "\n",
    "def getAccuracy(y_true, y_pred, data_length):\n",
    "    \"\"\"\n",
    "        y_true : 원래 타겟 컬럼의 데이터를 넣어준다.\n",
    "        y_pred : 예측한 값을 넣어준다.\n",
    "        data_length : 예측한 데이터의 총 개수를 넣어준다.\n",
    "    \"\"\"\n",
    "    display(Markdown(\"##### Accuracy : \" + str(round(np.trace(confusion_matrix(y_true, y_pred))/data_length, 4))))\n",
    "\n",
    "def fitNaiveBayesModel(X, y):\n",
    "    return MultinomialNB().fit(X, y)\n",
    "\n",
    "def fitXGBClassifier(X, y, n_estimators=100, max_depth=2):\n",
    "    return xgboost.XGBClassifier(n_estimators = n_estimators, max_depth = max_depth)\n",
    "\n",
    "def saveDataFrameToCsv(df, fileName, is_submission_df = False, idx = False):\n",
    "    # fl을 이용해 만든 데이터프레임은 용량이 1Gb정도 되므로 저장시간이 8~9분 걸린다.\n",
    "    \"\"\"\n",
    "        넘겨준 df를 filename + 년월일시간분 의 format으로 이루어진 이름의 파일로 생성해준다.\n",
    "        index를 True로 넘겨주면 저장할 때 아규먼트로 index=True를 넣어주게 된다.\n",
    "        is_submission_df 를 통해서 submission은 다른 폴더로 저장시킨다.\n",
    "    \"\"\"\n",
    "    fileName += \"_\" + datetime.now().strftime(\"%Y%m%d%H%M\") + \".csv\"\n",
    "    if is_submission_df:\n",
    "        fileName = \"/Submission_models/\" + fileName\n",
    "    else:\n",
    "        fileName = \"/Feature_matrix/\" + fileName\n",
    "    return df.to_csv(fileName, index = idx)\n",
    "\n",
    "def sendSlackDm(url, text):\n",
    "    \"\"\"\n",
    "        Parameter :\n",
    "            각자 받은 url을 넣어준다.\n",
    "            text에는 보낼 글 내용\n",
    "    \"\"\"\n",
    "    webhook_url = url\n",
    "    slack_data = {'text': text}\n",
    "    response = requests.post(\n",
    "        webhook_url,\n",
    "        data=json.dumps(slack_data),\n",
    "        headers={'Content-Type': 'application/json'}\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(\n",
    "            'Request to slack returned an error %s, the response is:\\n%s'%(response.status_code, response.text)\n",
    "    )\n",
    "        \n",
    "def saveModelObjectAsPickle(model, fileName)\n",
    "    filename = \"/Model_pkl/\" + fileName\n",
    "    joblib.dump(model, fileName)\n",
    "    display(Markdown(\"##### Done!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Dummy타입으로 만든 컬럼 명 : DepartmentDescription"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### ScanCount는 양수만 사용"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Train df 만드는 중"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Null 컬럼을 만듬"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> 위 정보들을 Display하는 이유는 이번 FeatureMatrix를 사용한 모델에 Fit할 Test 모델 만들 때 같은 전처리를 하기 위해서다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 s, sys: 1.06 s, total: 3.51 s\n",
      "Wall time: 3.52 s\n"
     ]
    }
   ],
   "source": [
    "%time df_we_wanted = make_df_we_wanted(df_train, df_train, df_test, is_need_null_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Dummy타입으로 만든 컬럼 명 : FinelineNumber"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### ScanCount는 양수만 사용"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Train df 만드는 중"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Null 컬럼을 만듬"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> 위 정보들을 Display하는 이유는 이번 FeatureMatrix를 사용한 모델에 Fit할 Test 모델 만들 때 같은 전처리를 하기 위해서다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### 이 작업은 데이터프레임을 반환하는     함수이므로 변수로 받아주셔야합니다!! 오래 걸리는 작업이므로 지금 실수하셨다면 빨리 커널 종료하고 다시 시도해주세요."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0명 진행됨. 아직 95674명 데이터 남음.\n",
      "5000명 진행됨. 아직 90674명 데이터 남음.\n",
      "10000명 진행됨. 아직 85674명 데이터 남음.\n",
      "15000명 진행됨. 아직 80674명 데이터 남음.\n",
      "20000명 진행됨. 아직 75674명 데이터 남음.\n",
      "25000명 진행됨. 아직 70674명 데이터 남음.\n",
      "30000명 진행됨. 아직 65674명 데이터 남음.\n",
      "35000명 진행됨. 아직 60674명 데이터 남음.\n",
      "40000명 진행됨. 아직 55674명 데이터 남음.\n",
      "45000명 진행됨. 아직 50674명 데이터 남음.\n",
      "50000명 진행됨. 아직 45674명 데이터 남음.\n",
      "55000명 진행됨. 아직 40674명 데이터 남음.\n",
      "60000명 진행됨. 아직 35674명 데이터 남음.\n",
      "65000명 진행됨. 아직 30674명 데이터 남음.\n",
      "70000명 진행됨. 아직 25674명 데이터 남음.\n",
      "75000명 진행됨. 아직 20674명 데이터 남음.\n",
      "80000명 진행됨. 아직 15674명 데이터 남음.\n",
      "85000명 진행됨. 아직 10674명 데이터 남음.\n",
      "90000명 진행됨. 아직 5674명 데이터 남음.\n",
      "95000명 진행됨. 아직 674명 데이터 남음.\n",
      "CPU times: user 11min 53s, sys: 1min 55s, total: 13min 48s\n",
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%time df_we_wanted_fl = make_df_we_wanted(df_train, df_train, df_test, dummie_col=\"FinelineNumber\", is_need_null_column=True)\n",
    "sendSlackDm(slack_url, \"Done train fl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Dummy타입으로 만든 컬럼 명 : DepartmentDescription"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### ScanCount는 양수만 사용"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Test df 만드는 중"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Null 컬럼을 만듬"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2.28 s, sys: 1 s, total: 3.28 s\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%time df_test_we_wanted = make_df_we_wanted(df_test, df_train, df_test, is_test_df=True, is_need_null_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Dummy타입으로 만든 컬럼 명 : FinelineNumber"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### ScanCount는 양수만 사용"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Test df 만드는 중"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Null 컬럼을 만듬"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "##### 이 작업은 데이터프레임을 반환하는     함수이므로 변수로 받아주셔야합니다!! 오래 걸리는 작업이므로 지금 실수하셨다면 빨리 커널 종료하고 다시 시도해주세요."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0명 진행됨. 아직 95674명 데이터 남음.\n",
      "5000명 진행됨. 아직 90674명 데이터 남음.\n",
      "10000명 진행됨. 아직 85674명 데이터 남음.\n",
      "15000명 진행됨. 아직 80674명 데이터 남음.\n",
      "20000명 진행됨. 아직 75674명 데이터 남음.\n",
      "25000명 진행됨. 아직 70674명 데이터 남음.\n",
      "30000명 진행됨. 아직 65674명 데이터 남음.\n",
      "35000명 진행됨. 아직 60674명 데이터 남음.\n",
      "40000명 진행됨. 아직 55674명 데이터 남음.\n",
      "45000명 진행됨. 아직 50674명 데이터 남음.\n",
      "50000명 진행됨. 아직 45674명 데이터 남음.\n",
      "55000명 진행됨. 아직 40674명 데이터 남음.\n",
      "60000명 진행됨. 아직 35674명 데이터 남음.\n",
      "65000명 진행됨. 아직 30674명 데이터 남음.\n",
      "70000명 진행됨. 아직 25674명 데이터 남음.\n",
      "75000명 진행됨. 아직 20674명 데이터 남음.\n",
      "80000명 진행됨. 아직 15674명 데이터 남음.\n",
      "85000명 진행됨. 아직 10674명 데이터 남음.\n",
      "90000명 진행됨. 아직 5674명 데이터 남음.\n",
      "95000명 진행됨. 아직 674명 데이터 남음.\n",
      "CPU times: user 11min 37s, sys: 2min, total: 13min 37s\n",
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%time df_test_we_wanted_fl = make_df_we_wanted(df_test, df_train, df_test, dummie_col=\"FinelineNumber\", is_test_df=True, is_need_null_column=True)\n",
    "sendSlackDm(slack_url, \"Done test fl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 78)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_we_wanted.columns), len(df_test_we_wanted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5364, 5363)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_we_wanted_fl.columns), len(df_test_we_wanted_fl.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
